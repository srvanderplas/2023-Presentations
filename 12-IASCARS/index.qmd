---
from: markdown+emoji
format: 
  revealjs:
    navigation-mode: vertical
    logo: ../libs/unl/N.svg
    theme: ../libs/unl/inverse.scss
    includes:
      in_header: ../libs/unl/header.html
    lib_dir: ../libs/unl/
---

# Problem Overview {.center .inverse}

## Footwear Forensics

::: columns
::: column
![Shoe Scanner Outside Setup](images/2021-June-1.png)
:::

::: {.column .smaller .r-fit-text}
-   Collect images of shoe soles from the population using the scanner

-   Identify features in the tread patterns w/ computer vision

-   Generate a local database of common pattern features

-   Characterize frequency of a new shoe w/ random match probability computed from database
:::
:::

::: smaller
> [Quantifying the frequency of shoes in a local population is an unsolveable problem]{.emph .cerulean} - Leslie Hammer, [Hammer Forensics](https://hammerforensics.com/), March 2018
:::

::: notes
This is the overall approach we'd planned to use for this project. We set out specifically to use features identifiable by examiners (so using the vocabulary they use to identify footwear features) with computer vision/deep learning algorithms. Our thought was that if we used features familiar to examiners, the model would be more usable by examiners -- and thus, more likely to be adopted in practice. This is admittedly a different take on explainable AI but it is important in forensics that examiners understand what the algorithm is doing (at a basic level) so that they can explain its use in court, and that requires some familiarity. As the way these algorithms work is very foreign, we decided that it would be easier to work with features that were familiar. Sounds simple enough, right?
:::

## Footwear Forensics

-   Other researchers use the [output]{.emph .red} from the CNN\
    (without a trained model head)

    -   hard to explain to practitioners

    -   hard to understand meaning

    -   for models to be accepted in forensics, they need to be explainable!

![](images/let_me_explain.gif){width="50%" fig-align="center"}

::: notes
Specifically, when we're done with this whole project -- the scanner and the model -- we have to convince forensic examiners that it's worth using. And forensic examiners are a great bunch of people - for the most part, they're very dedicated, very smart, and have a lot of expertise in their field. But their field is definitively not math, and most of them are not what we'd call quantitatively oriented. So that is a big limitation on what we can do with neural networks.

A lot of previous work with neural networks in forensic pattern analysis uses features directly from the model base, without a trained model head. These features don't make sense to practitioners, so the entire research project is dead before it starts, because we have to get examiners to adopt this stuff before it makes any practical impact.

We want to avoid this process, so we specifically set up this model to spit out features that we can all describe -- lines, circles, bowties, chevrons. We have to work within the confines of human language and our model has to spit out features that are explainable to examiners and can be generated by examiners.
:::

## Our Assumption in 2018

::: {layout="[[-1,2,2,-1]]"}
[![African Elephant](https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/African_Elephant_%28Loxodonta_africana%29_bull_%2831100819046%29.jpg/1024px-African_Elephant_%28Loxodonta_africana%29_bull_%2831100819046%29.jpg){fig-alt="Picture of an African Elephant" fig-align="center" width="30%"}](https://commons.wikimedia.org/wiki/File:African_Elephant_%28Loxodonta_africana%29_bull_%2831100819046%29.jpg)

[![Asian Elephant](https://upload.wikimedia.org/wikipedia/commons/9/98/Elephas_maximus_%28Bandipur%29.jpg){fig-alt="A picture of an Asian elephant" fig-align="center" width="30%"}](https://commons.wikimedia.org/wiki/File:Elephas_maximus_(Bandipur).jpg)
:::

[If models can differentiate between types of elephants, they can identify shapes... right?]{.smaller}

::: fragment
::: {layout-ncol="3"}
![Circles](images/circle_examples_horiz.png)

![Quads](images/quad_examples_horiz.png)

![???](images/dc_circle_quad_horiz.png)
:::
:::

:::

::: notes
Neural networks can differentiate between African and Asian elephants. I can't even do that... They'll be able to differentiate between a circle and a square easily.
:::

## XKCD: Tasks

[![In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they\'d have the problem solved by the end of the summer. Half a century later, we\'re still working on it.](https://imgs.xkcd.com/comics/tasks_2x.png){fig-align="center"}](https://xkcd.com/1425/)

## Complication: Different CV Models

We can reasonably pose this problem in 3 different ways:

::: {layout-ncol="3"}
![Classification: same-size regions labeled with one or more classes](images/classification-shoe-example.png)

![Object Detection: Propose a bounding box and label for each object in an image](images/obj-detection-shoe-example.png)

![Image segmentation: find regions of the image and label each region](images/segmentation-shoe-example.png)
:::

Each method requires a different labeling schema, annotation method, and data format

# [In Search of Human-Friendly Model Output]{.smaller} {.center .inverse}

[(What we've tried so far)]{.cerulean .emph .fragment}

## Initial Approach (\~2019) {.r-fit-text}

-   Use VGG16 to classify 256x256 px chunks of images

-   Goal is to label the entire chunk with one or more classes

![VGG16 Shoe Example approach](images/vgg16-shoe-example.png){fig-align="center" width="75%"}

-   Hard to integrate predictions into the main image

## Initial Approach (\~2019) {.r-fit-text}

![Not *terrible* but a lot of class confusion between e.g. Circles & Text, Quad & Polygon, Quad & Triangle](images/ConfMatrix-1.png){width="50%" fig-align="center"}

## Synthetic Data Test (2020) {.r-fit-text}

-   If we create different shapes, can a neural network differentiate them?

![Class Examples](images/Synthetic-Images.png){width="auto" fig-align="center"}

## Synthetic Data Test (2020) {.r-fit-text}

![](images/ConfMatrix2-1.png){width="50%" fig-align="center"}

## Synthetic Data Test (2020) {.r-fit-text}

| Image                                                   | Circle | 3   | 4   | 5   | 6   | 7   | 8   | 9   | Star |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| ![](images/spiral.png){width="80%" fig-align="center"}  | .9999  | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0    |
| ![](images/sqircle.png){width="80%" fig-align="center"} | .9999  | 0   | 0   | 0   | 0   | 0   | 0   | 0   | 0    |

::: fragment
-   Shoe data is complicated to label

-   Predictions made on ambiguous data don't work as well as we'd like
:::

## Object Detection (2021-2023)

![Object Detection: Propose a bounding box and label for each object in an image](images/obj-detection-shoe-example.png)

-   re-encode labels using a different data format

-   Toolkit:

    -   Started with FastAI, but had terrible support/documentation
    -   Eventually rewrote everything in PyTorch

## Fundamental Problem

![What shape is in the box? Text? Circle? Triangle? Star?](images/Ugg-ambiguous-image.png){width="80%" fig-align="center"}

::: smaller
-   Neural networks are trained on millions of human-annotated [**photos**]{.emph .purple}

-   Even shoe soles are artificial relative to a natural scene

-   Networks weren't trained on the artificial patterns or layouts in shoe soles

-   Labeling is fraught with [**errors**]{.emph .red} and [**incomplete information**]{.emph .cerulean}

-   Labeling schema are very complex & must account for human perception
:::

::: notes
If we look at the shape in the box, there is not actually a circle there - instead, there are triangles arranged inside an invisible circle, with text in the center inside another invisible circle made up of the points of the triangles. This is ... hard to label. Our undergrad RAs (and even the researchers) aren't sure how to deal with this.

We can get useful features out of existing NNs if we sacrifice interpretability/explainability, but examiners aren't likely to use those tools in practice - they're better used in database searches and other situations that don't require examiners to apply them.

It seems like we don't have sufficient data to adapt to the diverse patterns we see in real shoe treads: we need thousands or millions of perfectly labeled images to train or adapt a NN model. The only real solution to this problem is to do something somewhat different - we simply can't generate that volume of data through human labeling alone (I don't have Google's budget).
:::

# [Approaching the Problem Backwards]{.smaller} {.center .inverse}

![](https://media.tenor.com/MmEDl-SG2eoAAAAd/dog-backup.gif){width="50%" fig-align="center"}

## [Approaching the Problem Backwards]{.smaller} {.r-fit-text}

1.  Generate a large library of synthetic data

    -   pre-labeled

    -   complex characteristics

    -   Train preliminary model

2.  Run 2D patterns through an existing network to generate more realistic 3D images

    -   Train 2nd-gen model

## [Approaching the Problem Backwards]{.smaller} {.r-fit-text}

2.  Run 2D patterns through an existing network to generate more realistic 3D images

    -   Train 2nd-gen model

3.  Train on marketing-quality pictures labeled by humans

    -   Update 2nd gen model (transfer learning)

## [Approaching the Problem Backwards]{.smaller} {.r-fit-text}

3.  Train on marketing-quality pictures labeled by humans

    -   Update 2nd gen model (transfer learning)

4.  Train on Scanner Photos

    -   Update 3rd gen model weights\
        (account for lower-quality photos)

::: fragment
Measure performance/accuracy changes over time on a consistent set of stimuli created from real shoes
:::

## Synthetic Pattern Generation

-   Use [SoleMate style labeling](SoleMate.pdf) for examiner familiarity\
    [(Much more complex coding scheme)]{.fragment .emph .cerulean}

::: {layout-ncol="4"}
![](images/SoleMate1.png)

![](images/SoleMate2.png)

![](images/SoleMate3.png)

![](images/SoleMate4.png)
:::

## Synthetic Pattern Generation

### Regions of distinct patterns

::: {layout-ncol="4"}
![](images/Regions_Athletic.svg)

![](images/Regions_Athletic_2.svg)

![](images/Regions_Flat.svg)

![](images/Regions_Flat_2.svg)

![](images/Regions_Flat_3.svg)

![](images/Regions_Heel_2.svg)

![](images/Regions_Work_2.svg)

![](images/Regions_Work.svg)
:::

::: notes
The basic idea I have is to use real shoes to generate rough "layouts" that are common across different shoe types.
:::

## Synthetic Data Generation

### Different Patterns

::: {layout-ncol="4"}
![Snowflake](images/Pattern_Snowflake.svg)

![Hexagon Open Circle](images/Pattern_Open_Circle_Hexagon_Tile.svg)

![Stud](images/Pattern_Stud_1.svg)

![Target Solid Circle](images/Pattern_Target_Solid_2.svg)

![Circle Bar Across](images/Pattern_Circle_Bar_Across.svg)

![Solid Circle Array](images/Pattern_Solid_Circle_Array.svg)

![Targets with Arcs](images/Pattern_Target_Arc.svg)

![6-pointed star](images/Pattern_Star_6.svg)
:::

::: notes
Then, for each layout region, we can choose a pattern and intersect the pattern with that region to produce a layout that covers the whole area. We can of course apply transforms (skew, stretch, rotate, etc.) to this to provide more options as necessary.
:::

## Synthetic Data Generation

### Shoe Outlines

::: {layout-ncol="4"}
![](images/Outline_Athletic.svg)

![](images/Outline_Athletic_2.svg)

![](images/Outline_Work.svg)

![](images/Outline_Work_2.svg)

![](images/Outline_Heel.svg)

![](images/Outline_Heel_2.svg)

![](images/Outline_Flat.svg)

![](images/Outline_Flat_2.svg)

![](images/Outline_Flat_3.svg)
:::

::: notes
Then, at the end, we can crop to the outline of the shoe; these are traced from real shoes and simplified.

This should give us a very large library of training data that we can then manipulate.
:::

## Synthetic Data Generation

::: {layout-ncol="2"}
![](images/Ex_1_Athletic_Bars.svg)

![](images/Ex_2_Athletic_Complex.svg)

![](images/Ex_3_Heel_Lines.svg)

![](images/Ex4_Work_Arc.svg)
:::

## Synthetic Data Generation {.smaller}

::: columns
::: {.column width="48%"}
### Advantages

-   SVGs can include metadata

-   Easy scaling

-   SVG intersection operations will automatically mark partial objects

-   Flexible data format:

    -   Region segmentation
    -   Object Detection
    -   Object Classification\
        [all generated from same source data]{.emph .cerulean .fragment}
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
### Disadvantages

-   Manual SVG creation\
    [(52 images $\approx$ 8h )]{.smaller}

-   Creating a library to generate data

-   3D rendering after 2D stage:

    -   digital via OpenSCAD + SVG?
    -   Can apply different surface colors

-   Lots of work required before we start in on photos
:::
:::

::: notes
And as each one of these layers is an SVG, we will have pre-labeled data (because we can label the SVG objects as we create them) and we'll be able to figure out if the object is occluded, partially included, or fully included. So we'll have pre-existing region segmentation and object labels... and hopefully we'll have less variability in these object labels than we would get having an undergrad manually label data.

Examiners told us in the 2021 IAI workshop that they tended to use multiple labels to describe a feature that was ambiguous... that is hard to train a network on, so we have decided to approach this from a bottom-up rather than top-down approach.
:::

## End Goal

### Human Friendly Model Outputs

-   Familiar features for database search

-   Data quality flexibility:

    -   Messy photos for database creationi
    -   Neat images for search

-   Goal: reliable estimates of random match probability [RMP: the probability that someone in the area has a shoe with similar characteristics.]{.emph .cerulean .fragment}

# Questions? {.center .inverse}

# Acknowledgements {.center .inverse}

::: smaller
This work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.
:::

Students who have worked on this project:

-   Muxin Hua (2022-)
-   Jayden Stack (2021-2022)
-   Miranda Tilton (2018-2019)
